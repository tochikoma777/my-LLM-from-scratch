# GPT è¯­è¨€æ¨¡å‹å®ç°ä¸å¾®è°ƒé¡¹ç›®

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

æœ¬é¡¹ç›®ä»é›¶å¼€å§‹å®ç°äº†å®Œæ•´çš„ GPTï¼ˆGenerative Pre-trained Transformerï¼‰è¯­è¨€æ¨¡å‹æ¶æ„ï¼ŒåŒ…å«é¢„è®­ç»ƒã€æƒé‡åŠ è½½ã€æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Fine-tuningï¼‰ç­‰å®Œæ•´æµç¨‹ã€‚ä»£ç ç»“æ„æ¸…æ™°ã€æ³¨é‡Šè¯¦å°½ï¼Œé€‚åˆç”¨äºå­¦ä¹  Transformer æ¶æ„åŸç†æˆ–ä½œä¸º NLP é¡¹ç›®çš„åŸºç¡€æ¡†æ¶ã€‚

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç‰¹ç‚¹](#-é¡¹ç›®ç‰¹ç‚¹)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
- [è¯¦ç»†è¯´æ˜](#-è¯¦ç»†è¯´æ˜)
  - [1. æ•°æ®é¢„å¤„ç†](#1-æ•°æ®é¢„å¤„ç†)
  - [2. æ¨¡å‹æ¶æ„](#2-æ¨¡å‹æ¶æ„)
  - [3. è®­ç»ƒæµç¨‹](#3-è®­ç»ƒæµç¨‹)
  - [4. é¢„è®­ç»ƒæƒé‡åŠ è½½](#4-é¢„è®­ç»ƒæƒé‡åŠ è½½)
  - [5. æŒ‡ä»¤å¾®è°ƒ](#5-æŒ‡ä»¤å¾®è°ƒ)
- [é…ç½®è¯´æ˜](#-é…ç½®è¯´æ˜)
- [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#-æ€§èƒ½ä¼˜åŒ–å»ºè®®)
- [å‚è€ƒèµ„æ–™](#-å‚è€ƒèµ„æ–™)
- [è®¸å¯è¯](#-è®¸å¯è¯)

## âœ¨ é¡¹ç›®ç‰¹ç‚¹

- **å®Œæ•´å®ç°**: ä»é›¶å®ç° GPT æ¶æ„ï¼ŒåŒ…æ‹¬å¤šå¤´æ³¨æ„åŠ›ã€å±‚å½’ä¸€åŒ–ã€å‰é¦ˆç½‘ç»œã€Transformer å—ç­‰æ‰€æœ‰æ ¸å¿ƒç»„ä»¶
- **ç”Ÿäº§å°±ç»ª**: æ”¯æŒä» OpenAI å®˜æ–¹åŠ è½½ GPT-2 é¢„è®­ç»ƒæƒé‡ï¼ˆ124M/355M/774M/1558Mï¼‰
- **æŒ‡ä»¤å¾®è°ƒ**: å®Œæ•´çš„æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Fine-tuningï¼‰å®ç°ï¼Œæ”¯æŒ Alpaca æ ¼å¼æ•°æ®
- **çµæ´»é…ç½®**: é€šè¿‡é…ç½®æ–‡ä»¶è½»æ¾è°ƒæ•´æ¨¡å‹å¤§å°å’Œè®­ç»ƒå‚æ•°
- **è¯¦å°½æ³¨é‡Š**: æ¯è¡Œå…³é”®ä»£ç å‡é…æœ‰è¯¦ç»†ä¸­æ–‡æ³¨é‡Šï¼Œä¾¿äºå­¦ä¹ å’Œç†è§£

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
Python >= 3.8
PyTorch >= 2.0
CUDA >= 11.0 (æ¨èï¼Œç”¨äºGPUåŠ é€Ÿ)
```

### å®‰è£…ä¾èµ–

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install tiktoken matplotlib tqdm tensorflow  # ç”¨äºåŠ è½½TFæ£€æŸ¥ç‚¹
```

### 1. ä»å¤´è®­ç»ƒå°å‹ GPT

```bash
python module_train.py
```

è¿™å°†ä¸‹è½½ç¤ºä¾‹æ–‡æœ¬ï¼ˆã€ŠThe Verdictã€‹çŸ­ç¯‡å°è¯´ï¼‰ï¼Œè®­ç»ƒä¸€ä¸ª 124M å‚æ•°çš„å°å‹ GPT æ¨¡å‹ã€‚

ğŸ“‹è¿è¡Œæ•ˆæœï¼š
![alt text](image.png)
### 2. åŠ è½½é¢„è®­ç»ƒ GPT-2 ç”Ÿæˆæ–‡æœ¬

```bash
python module_load_param.py
```

è‡ªåŠ¨ä¸‹è½½ OpenAI GPT-2 æƒé‡å¹¶ç”Ÿæˆæ–‡æœ¬ã€‚æ”¯æŒé€‰æ‹©ä¸åŒå¤§å°çš„æ¨¡å‹ï¼š
- `124M`: å°å‹ï¼ˆé»˜è®¤ï¼Œé€‚åˆæ™®é€šGPUï¼‰
- `355M`: ä¸­å‹ï¼ˆéœ€è¦ 8GB+ VRAMï¼‰
- `774M`: å¤§å‹ï¼ˆéœ€è¦ 16GB+ VRAMï¼‰
- `1558M`: XLï¼ˆéœ€è¦ 24GB+ VRAMï¼‰

ğŸ“‹è¿è¡Œæ•ˆæœï¼š
![alt text](image-1.png)
### 3. æŒ‡ä»¤å¾®è°ƒ

```bash
# å®Œæ•´å¾®è°ƒï¼ˆä½¿ç”¨ GPT-2 Mediumï¼Œçº¦éœ€ 30 åˆ†é’Ÿï¼‰
python module_fine_tuning.py

# æµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨æå°æ¨¡å‹ï¼Œå¿«é€ŸéªŒè¯ä»£ç ï¼‰
python module_fine_tuning.py --test_mode
```

ğŸ“‹è¿è¡Œæ•ˆæœï¼š
![alt text](image-2.png)
## ğŸ“ é¡¹ç›®ç»“æ„

```
modules/
â”œâ”€â”€ data_preprocess.py              # æ•°æ®é¢„å¤„ç†ï¼šåˆ†è¯ã€æ•°æ®é›†æ„å»º
â”œâ”€â”€ generate_text_simple.py         # åŸºç¡€æ–‡æœ¬ç”Ÿæˆï¼ˆè´ªå©ªè§£ç ï¼‰
â”œâ”€â”€ language_module.py              # GPT æ¨¡å‹æ¶æ„å®šä¹‰
â”œâ”€â”€ module_train.py                 # è®­ç»ƒæµç¨‹ä¸è¯„ä¼°
â”œâ”€â”€ module_load_param.py            # é¢„è®­ç»ƒæƒé‡åŠ è½½ä¸å¢å¼ºç”Ÿæˆ
â”œâ”€â”€ module_fine_tuning.py           # æŒ‡ä»¤å¾®è°ƒå®ç°
â”œâ”€â”€ README.md                       # æœ¬æ–‡ä»¶
â””â”€â”€ requirements.txt                # ä¾èµ–åˆ—è¡¨
```

## ğŸ” è¯¦ç»†è¯´æ˜

### 1. æ•°æ®é¢„å¤„ç†

**æ–‡ä»¶**: `data_preprocess.py`

**æ ¸å¿ƒåŠŸèƒ½**:
- ä½¿ç”¨ `tiktoken`ï¼ˆGPT-2 å®˜æ–¹åˆ†è¯å™¨ï¼‰è¿›è¡Œ BPE ç¼–ç 
- æ»‘åŠ¨çª—å£åˆ‡åˆ†é•¿æ–‡æœ¬ä¸ºå›ºå®šé•¿åº¦åºåˆ—
- æ„å»º `Input-Target` å¯¹ç”¨äºè‡ªå›å½’è®­ç»ƒ

**å…³é”®å‚æ•°**:
```python
max_length = 256    # åºåˆ—é•¿åº¦ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰
stride = 128        # æ»‘åŠ¨æ­¥é•¿ï¼ˆæ§åˆ¶æ•°æ®é‡å åº¦ï¼‰
batch_size = 8      # æ‰¹æ¬¡å¤§å°
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from data_preprocess import create_dataloader_v1

with open("data.txt", "r") as f:
    text = f.read()

dataloader = create_dataloader_v1(
    text, 
    batch_size=4, 
    max_length=256, 
    stride=128
)
```

### 2. æ¨¡å‹æ¶æ„

**æ–‡ä»¶**: `language_module.py`

å®ç°äº†å®Œæ•´çš„ GPT æ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š

| ç»„ä»¶ | ç±»å | è¯´æ˜ |
|------|------|------|
| å¤šå¤´æ³¨æ„åŠ› | `MultiHeadAttention` | å› æœè‡ªæ³¨æ„åŠ›ï¼Œæ”¯æŒå¤šå¤´çš„å¹¶è¡Œè®¡ç®— |
| å±‚å½’ä¸€åŒ– | `LayerNorm` | Pre-LN ç»“æ„ï¼Œç¨³å®šæ·±å±‚ç½‘ç»œè®­ç»ƒ |
| æ¿€æ´»å‡½æ•° | `GELU` | é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼Œå¹³æ»‘éçº¿æ€§ |
| å‰é¦ˆç½‘ç»œ | `FeedForward` | æ‰©å±•-æ”¶ç¼©ç»“æ„ï¼ˆ4x ç»´åº¦ï¼‰ |
| Transformerå— | `TransformerBlock` | æ³¨æ„åŠ› + FFN + æ®‹å·®è¿æ¥ |
| GPTæ¨¡å‹ | `GPTModel` | å®Œæ•´æ¨¡å‹ï¼ŒåŒ…å«åµŒå…¥ã€Transformerå †å ã€è¾“å‡ºå¤´ |

**é…ç½®ç¤ºä¾‹**ï¼ˆGPT-2 Smallï¼‰:
```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,      # è¯æ±‡è¡¨å¤§å°
    "context_length": 1024,   # æœ€å¤§åºåˆ—é•¿åº¦
    "emb_dim": 768,           # åµŒå…¥ç»´åº¦
    "n_heads": 12,            # æ³¨æ„åŠ›å¤´æ•°
    "n_layers": 12,           # Transformerå±‚æ•°
    "drop_rate": 0.1,         # Dropoutæ¦‚ç‡
    "qkv_bias": False         # QKVåç½®
}
```

### 3. è®­ç»ƒæµç¨‹

**æ–‡ä»¶**: `module_train.py`

**ç‰¹æ€§**:
- **è‡ªåŠ¨è®¾å¤‡æ£€æµ‹**: è‡ªåŠ¨ä½¿ç”¨ CUDA GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
- **å®šæœŸè¯„ä¼°**: æ¯ N æ­¥è®¡ç®—éªŒè¯æŸå¤±ï¼Œç›‘æ§è¿‡æ‹Ÿåˆ
- **æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹**: æ¯ä¸ª epoch ç»“æŸç”Ÿæˆæ–‡æœ¬ï¼Œç›´è§‚è§‚å¯Ÿè®­ç»ƒæ•ˆæœ
- **è®­ç»ƒå¯è§†åŒ–**: è‡ªåŠ¨ç”ŸæˆæŸå¤±æ›²çº¿å›¾ï¼ˆæ”¯æŒåŒåæ ‡è½´ï¼šepochs å’Œ tokensï¼‰

**è®­ç»ƒå¾ªç¯æ ¸å¿ƒé€»è¾‘**:
```python
for epoch in range(num_epochs):
    for batch in train_loader:
        optimizer.zero_grad()           # 1. æ¢¯åº¦æ¸…é›¶
        loss = calc_loss_batch(...)     # 2. å‰å‘ä¼ æ’­
        loss.backward()                 # 3. åå‘ä¼ æ’­
        optimizer.step()                # 4. å‚æ•°æ›´æ–°
        
        if step % eval_freq == 0:
            evaluate_model(...)         # å®šæœŸè¯„ä¼°
```

### 4. é¢„è®­ç»ƒæƒé‡åŠ è½½

**æ–‡ä»¶**: `module_load_param.py`

**åŠŸèƒ½**:
- è‡ªåŠ¨ä» OpenAI å®˜æ–¹ä»“åº“ä¸‹è½½ GPT-2 æƒé‡ï¼ˆTensorFlow æ£€æŸ¥ç‚¹æ ¼å¼ï¼‰
- è·¨æ¡†æ¶æƒé‡è½¬æ¢ï¼ˆTensorFlow â†’ PyTorchï¼‰
- ç²¾ç¡®çš„æƒé‡æ˜ å°„ï¼ˆå¤„ç† QKV åˆ†å‰²ã€ç»´åº¦è½¬ç½®ç­‰å·®å¼‚ï¼‰
- å¢å¼ºç‰ˆç”Ÿæˆå‡½æ•°ï¼ˆæ”¯æŒ Temperature é‡‡æ ·å’Œ Top-k ç­›é€‰ï¼‰

**ç”Ÿæˆå‚æ•°è¯´æ˜**:
```python
generate(
    model=model,
    idx=input_ids,
    max_new_tokens=100,
    temperature=0.8,    # <1.0 ä¿å®ˆï¼Œ>1.0 åˆ›é€ æ€§
    top_k=40,           # ä»…ä»æ¦‚ç‡å‰40çš„tokené‡‡æ ·
    eos_id=50256        # é‡åˆ°ç»“æŸç¬¦åœæ­¢
)
```

### 5. æŒ‡ä»¤å¾®è°ƒ

**æ–‡ä»¶**: `module_fine_tuning.py`

**æ•°æ®æ ¼å¼**ï¼ˆAlpaca é£æ ¼ï¼‰:
```json
{
    "instruction": "å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡",
    "input": "Hello, how are you?",
    "output": "ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ"
}
```

**å…³é”®æŠ€æœ¯**:
- **è‡ªå®šä¹‰ Collate Function**: å¤„ç†å˜é•¿åºåˆ—ï¼Œå±è”½å¡«å…… token çš„æŸå¤±
- **æŸå¤±å±è”½**: åªåœ¨ `### Response:` éƒ¨åˆ†è®¡ç®—æŸå¤±ï¼Œé¿å…å­¦ä¹ è¾“å…¥æ¨¡æ¿
- **æƒé‡å†»ç»“é€‰é¡¹**: å¯é€‰æ‹©æ€§å†»ç»“åº•å±‚å‚æ•°ï¼Œåªå¾®è°ƒé¡¶å±‚ï¼ˆä»£ç ä¸­å¯æ‰©å±•ï¼‰

**å¾®è°ƒç­–ç•¥**:
- ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆ5e-5ï¼Œæ¯”é¢„è®­ç»ƒå° 10-100 å€ï¼‰
- è¾ƒå°‘çš„è®­ç»ƒè½®æ¬¡ï¼ˆé€šå¸¸ 2-3 è½®å³å¯ï¼‰
- ä¿ç•™ç¬¬ä¸€ä¸ªå¡«å…… token çš„æŸå¤±ï¼Œè®©æ¨¡å‹å­¦ä¹ ä½•æ—¶åœæ­¢ç”Ÿæˆ

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹å¤§å°å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | åµŒå…¥ç»´åº¦ | å±‚æ•° | å¤´æ•° | æ˜¾å­˜éœ€æ±‚ |
|------|--------|----------|------|------|----------|
| Small (124M) | 124M | 768 | 12 | 12 | ~2GB |
| Medium (355M) | 355M | 1024 | 24 | 16 | ~6GB |
| Large (774M) | 774M | 1280 | 36 | 20 | ~12GB |
| XL (1558M) | 1.5B | 1600 | 48 | 25 | ~24GB |

### å…³é”®è¶…å‚æ•°è°ƒä¼˜å»ºè®®

| å‚æ•° | é¢„è®­ç»ƒå»ºè®® | å¾®è°ƒå»ºè®® | è¯´æ˜ |
|------|-----------|---------|------|
| learning_rate | 1e-4 ~ 1e-3 | 1e-5 ~ 1e-4 | å¾®è°ƒæ—¶ä½¿ç”¨æ›´å°å­¦ä¹ ç‡ |
| batch_size | æ ¹æ®æ˜¾å­˜æœ€å¤§åŒ– | 4-16 | è¶Šå¤§è¶Šç¨³å®šï¼Œä½†éœ€æ›´å¤šæ˜¾å­˜ |
| num_epochs | 10-100+ | 2-5 | å¾®è°ƒå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€æ—©åœ |
| weight_decay | 0.01-0.1 | 0.01-0.1 | L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |
| drop_rate | 0.1-0.2 | 0.0-0.1 | å¾®è°ƒæ—¶é€šå¸¸é™ä½ dropout |

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
æ·»åŠ ä»¥ä¸‹ä»£ç å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œå¯èŠ‚çœ 30-50% æ˜¾å­˜å¹¶åŠ é€Ÿè®­ç»ƒï¼š
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    loss = calc_loss_batch(...)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 2. æ¢¯åº¦ç´¯ç§¯
æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒè€Œä¸å¢åŠ æ˜¾å­˜å ç”¨ï¼š
```python
accumulation_steps = 4
for i, batch in enumerate(loader):
    loss = calc_loss_batch(...) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 3. æ¢¯åº¦è£å‰ª
é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¨³å®šè®­ç»ƒï¼š
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 4. ç¼–è¯‘æ¨¡å‹ (PyTorch 2.0+)
ä½¿ç”¨ `torch.compile` åŠ é€Ÿæ¨ç†ï¼š
```python
model = torch.compile(model)
```

## ğŸ“š å‚è€ƒèµ„æ–™

1. **åŸå§‹è®ºæ–‡**:
   - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
   - [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (Radford et al., 2018)
   - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (Brown et al., 2020, GPT-3)

2. **å®˜æ–¹èµ„æº**:
   - [OpenAI GPT-2](https://github.com/openai/gpt-2)
   - [tiktoken](https://github.com/openai/tiktoken) - å¿«é€Ÿ BPE åˆ†è¯å™¨

3. **æ¨èå­¦ä¹ èµ„æº**:
   - [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) - æœ¬é¡¹ç›®ä¸»è¦å‚è€ƒ
   - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - å¯è§†åŒ–è®²è§£

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE) å¼€æºï¼Œå…è®¸è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘ï¼Œä½†è¯·ä¿ç•™åŸå§‹ä½œè€…å£°æ˜ã€‚

---

**ä½œè€…**: [tochikoma777]  
**è”ç³»æ–¹å¼**: [brother.777.lei@gmail.com]  
**é¡¹ç›®åœ°å€**: https://github.com/tochikoma777/my-LLM-from-scratch